{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "train_df = getDF('train.json.gz')\n",
    "test_df = getDF('test_Helpful.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryID</th>\n",
       "      <th>categories</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women], [Clothing...</td>\n",
       "      <td>I655355328</td>\n",
       "      <td>U745881038</td>\n",
       "      <td>3.0</td>\n",
       "      <td>These are cute, but they are a little small.  ...</td>\n",
       "      <td>R115160670</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>Cute</td>\n",
       "      <td>1400544000</td>\n",
       "      <td>{'outOf': 0, 'nHelpful': 0}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women, Clothing, ...</td>\n",
       "      <td>I241092314</td>\n",
       "      <td>U023577405</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I love the look of this bra, it is what I want...</td>\n",
       "      <td>R800651687</td>\n",
       "      <td>02 7, 2013</td>\n",
       "      <td>Beautiful but size runs small</td>\n",
       "      <td>1360195200</td>\n",
       "      <td>{'outOf': 0, 'nHelpful': 0}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Wedding Party Gif...</td>\n",
       "      <td>I408260822</td>\n",
       "      <td>U441384838</td>\n",
       "      <td>3.0</td>\n",
       "      <td>it's better on a man's hand.I didn't find it v...</td>\n",
       "      <td>R345042616</td>\n",
       "      <td>05 13, 2014</td>\n",
       "      <td>Good price but...</td>\n",
       "      <td>1399939200</td>\n",
       "      <td>{'outOf': 2, 'nHelpful': 2}</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women, Clothing, ...</td>\n",
       "      <td>I770448753</td>\n",
       "      <td>U654041297</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Comfortable and easy to wear for a day of shop...</td>\n",
       "      <td>R875466866</td>\n",
       "      <td>05 25, 2014</td>\n",
       "      <td>Easy, breezy</td>\n",
       "      <td>1400976000</td>\n",
       "      <td>{'outOf': 0, 'nHelpful': 0}</td>\n",
       "      <td>14.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women, Plus-Size,...</td>\n",
       "      <td>I919238161</td>\n",
       "      <td>U096604734</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I'm quite small and the XS fits me like a regu...</td>\n",
       "      <td>R317526520</td>\n",
       "      <td>07 30, 2013</td>\n",
       "      <td>Great shirt</td>\n",
       "      <td>1375142400</td>\n",
       "      <td>{'outOf': 1, 'nHelpful': 1}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categoryID                                         categories      itemID  \\\n",
       "0           0  [[Clothing, Shoes & Jewelry, Women], [Clothing...  I655355328   \n",
       "1           0  [[Clothing, Shoes & Jewelry, Women, Clothing, ...  I241092314   \n",
       "2           0  [[Clothing, Shoes & Jewelry, Wedding Party Gif...  I408260822   \n",
       "3           0  [[Clothing, Shoes & Jewelry, Women, Clothing, ...  I770448753   \n",
       "4           0  [[Clothing, Shoes & Jewelry, Women, Plus-Size,...  I919238161   \n",
       "\n",
       "   reviewerID  rating                                         reviewText  \\\n",
       "0  U745881038     3.0  These are cute, but they are a little small.  ...   \n",
       "1  U023577405     4.0  I love the look of this bra, it is what I want...   \n",
       "2  U441384838     3.0  it's better on a man's hand.I didn't find it v...   \n",
       "3  U654041297     4.0  Comfortable and easy to wear for a day of shop...   \n",
       "4  U096604734     5.0  I'm quite small and the XS fits me like a regu...   \n",
       "\n",
       "   reviewHash   reviewTime                        summary  unixReviewTime  \\\n",
       "0  R115160670  05 20, 2014                           Cute      1400544000   \n",
       "1  R800651687   02 7, 2013  Beautiful but size runs small      1360195200   \n",
       "2  R345042616  05 13, 2014              Good price but...      1399939200   \n",
       "3  R875466866  05 25, 2014                   Easy, breezy      1400976000   \n",
       "4  R317526520  07 30, 2013                    Great shirt      1375142400   \n",
       "\n",
       "                       helpful  price  \n",
       "0  {'outOf': 0, 'nHelpful': 0}    NaN  \n",
       "1  {'outOf': 0, 'nHelpful': 0}    NaN  \n",
       "2  {'outOf': 2, 'nHelpful': 2}  19.99  \n",
       "3  {'outOf': 0, 'nHelpful': 0}  14.95  \n",
       "4  {'outOf': 1, 'nHelpful': 1}    NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "train_df['categories'] = train_df.categories.apply(lambda x: list(itertools.chain(*x))).apply(lambda x: [each.split(',') for each in x]).apply(lambda x: list(set(itertools.chain(*x))))\n",
    "test_df['categories'] = test_df.categories.apply(lambda x: list(itertools.chain(*x))).apply(lambda x: [each.split(',') for each in x]).apply(lambda x: list(set(itertools.chain(*x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_fit = train_df.categories.to_list()\n",
    "test_categories_fit = test_df.categories.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.MultiLabelBinarizer()\n",
    "\n",
    "lb.fit(categories_fit)\n",
    "train_cats = pd.DataFrame(lb.transform(categories_fit),columns = lb.classes_)\n",
    "test_cats = pd.DataFrame(lb.transform(test_categories_fit),columns = lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('categories',axis = 1,inplace = True)\n",
    "test_df.drop('categories',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_df,train_cats],axis = 1)\n",
    "test = pd.concat([test_df,test_cats],axis = 1)\n",
    "\n",
    "# Uncomment to run without categories\n",
    "# train = train_df\n",
    "# test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpfulNess_train = pd.DataFrame(train_df.helpful.apply(lambda x: [x['outOf'],x['nHelpful']]).to_list(),columns = ['outOf','nHelpful'])\n",
    "\n",
    "helpfulNess_test = pd.DataFrame(test_df.helpful.apply(lambda x: [x['outOf']]).to_list(),columns = ['outOf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train,helpfulNess_train],axis = 1)\n",
    "test = pd.concat([test,helpfulNess_test],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['helpful','reviewHash','reviewText','price'],axis = 1,inplace = True)\n",
    "test.drop(['helpful','reviewHash','reviewText','price'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = train.reviewTime.apply(lambda x: [each.replace(',','') for each in str(x).split(' ')]).apply(lambda x:x[0])\n",
    "day = train.reviewTime.apply(lambda x: [each.replace(',','') for each in str(x).split(' ')]).apply(lambda x:x[1])\n",
    "year = train.reviewTime.apply(lambda x: [each.replace(',','') for each in str(x).split(' ')]).apply(lambda x:x[2])\n",
    "\n",
    "month_test = test.reviewTime.apply(lambda x: [each.replace(',','') for each in str(x).split(' ')]).apply(lambda x:x[0])\n",
    "day_test = test.reviewTime.apply(lambda x: [each.replace(',','') for each in str(x).split(' ')]).apply(lambda x:x[1])\n",
    "year_test = test.reviewTime.apply(lambda x: [each.replace(',','') for each in str(x).split(' ')]).apply(lambda x:x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['reviewTime'],axis = 1,inplace = True)\n",
    "test.drop(['reviewTime'],axis = 1,inplace = True)\n",
    "train.drop(['unixReviewTime'],axis = 1,inplace = True)\n",
    "test.drop(['unixReviewTime'],axis = 1,inplace = True)\n",
    "\n",
    "train['day'] = day.astype('int')\n",
    "train['month'] = month.astype('int')\n",
    "train['year'] = year.astype('int')\n",
    "\n",
    "test['day'] = day_test.astype('int')\n",
    "test['month'] = month_test.astype('int')\n",
    "test['year'] = year_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a feature for how many words there are in a Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/akash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "train['textLength'] = train_df.reviewText.apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "test['textLength'] = test_df.reviewText.apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "\n",
    "## Summary length does not work as well\n",
    "\n",
    "#train['summaryLength'] = train_df.summary.apply(lambda x:len(x.split(' ')))\n",
    "#test['summaryLength'] = test_df.summary.apply(lambda x:len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['summary'],axis = 1,inplace = True)\n",
    "test.drop(['summary'], axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reviewText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items will never be new, but reviewers will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "l_item = LabelEncoder()\n",
    "l_item.fit(train.itemID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['itemID'] = l_item.transform(train.itemID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['itemID'] = l_item.transform(test.itemID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_review = LabelEncoder()\n",
    "l_review.fit(train.reviewerID)\n",
    "\n",
    "train['reviewerID'] = l_review.transform(train.reviewerID)\n",
    "#test['reviewerID'] = l_review.transform(test.reviewerID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l_review.classes_ = np.append(l_review.classes_,'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknowns = set(test.reviewerID).difference(l_review.classes_)\n",
    "test['reviewerID'] = test.reviewerID.apply(lambda x: 'Unknown' if x in unknowns else x)\n",
    "test['reviewerID'] = l_review.transform(test.reviewerID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf vectorizer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Taking only max 5000 features into account. \n",
    "count_vectorizer = CountVectorizer(stop_words=stopwords.words('english'),lowercase=True,max_features=5000)\n",
    "train_summary = count_vectorizer.fit_transform(train_df.summary)\n",
    "test_summary = count_vectorizer.transform(test_df.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No need to scale Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scalers = {}\n",
    "# for i in range(train.shape[1]):\n",
    "#     colname = train.columns[i]is\n",
    "#     scalers[colname] = StandardScaler()\n",
    "#     train[colname] = scalers[colname].fit_transform(train[colname].to_numpy().reshape(-1,1))\n",
    "    \n",
    "# for i in range(test.shape[1]):\n",
    "#     colname = test.columns[i]\n",
    "#     test[colname] = scalers[colname].transform(test[colname].to_numpy().reshape(-1,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_ = train['nHelpful']\n",
    "train = train.drop('nHelpful',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "train = scipy.sparse.csr_matrix(train.values)\n",
    "test = scipy.sparse.csr_matrix(test.values)\n",
    "\n",
    "# Comment to remove summary\n",
    "train = hstack((train,train_summary))\n",
    "test = hstack((test,test_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_csr_new,val_new, train_y , val_y = train_test_split(train,help_,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(verbose = 1)\n",
    "rf.fit(train_csr_new,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.179975"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(val_y,smooth_predictions(rf.predict(val_new)))\n",
    "#mean_absolute_error(scalers['nHelpful'].inverse_transform(val_y),smooth_predictions(scalers['nHelpful'].inverse_transform(rf.predict(val_new))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_predictions(preds):\n",
    "    import numpy as np\n",
    "    for i in range(0,preds.shape[0]):\n",
    "        x = preds[i]\n",
    "        if x % 1 > 0.5:\n",
    "            preds[i] = np.ceil(x) \n",
    "        else: \n",
    "            preds[i] = np.floor(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_predictions(preds,message):\n",
    "    outof = pd.DataFrame(test_df.helpful.apply(lambda x: [x['outOf']]).to_list(),columns = ['outOf'])['outOf']\n",
    "    predictions = pd.DataFrame([test_df.reviewerID,test_df.itemID,outof,preds]).T\n",
    "    predictions.rename(columns = {'reviewerID':0,'itemID':1,'Unnamed 0':2,'Unnamed 1':3},inplace = True)\n",
    "    predictions[0] = predictions[0] + '-' + predictions[1] + '-'\n",
    "    predictions[0] = predictions[0].apply(lambda x: x.strip())\n",
    "    predictions[0] = predictions[0] + predictions['outOf'].apply(lambda x: str(int(x)).strip())\n",
    "    predictions.drop([1,'outOf'],axis = 1 ,inplace = True)\n",
    "    predictions.rename(columns = {0:'userID-itemID-outOf',2:'prediction'},inplace = True)\n",
    "    predictions.to_csv('predictions_first.csv',sep= ',',index = False)\n",
    "    !kaggle competitions submit -c dse220 -f predictions_first.csv -m 'Simplest Random Forsts ,without price ,category and anything'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-4d19ef7aeded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Re-training with entire training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhelp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.values_from_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5442\u001b[0m         \"\"\"\n\u001b[1;32m   5443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5446\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mas_array\u001b[0;34m(self, transpose, items)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m             \u001b[0mitemmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Re-training with entire training data\n",
    "rf = RandomForestRegressor(n_estimators = 100 ,verbose = 1)\n",
    "rf.fit(train,help_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = smooth_predictions(rf.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_predictions(preds,'nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/akash/.kaggle/kaggle.json'\n",
      "100%|████████████████████████████████████████| 384k/384k [00:10<00:00, 37.5kB/s]\n",
      "Successfully submitted to DSE 220 - Project"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c dse220 -f predictions_latest.csv -m 'ngrams with n from 3 to 4!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
